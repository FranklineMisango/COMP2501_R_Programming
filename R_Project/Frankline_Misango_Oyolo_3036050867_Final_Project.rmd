
---
title: "COMP 2501 Project : Stock Ticker Research using NLP and Pre-Trained Hugging Face model" 
output:
  word_document: default
  html_document:
    df_print: paged
---

### 1. Installing the needed packages from Python 
```{r}

library(reticulate)
reticulate::py_install(c("transformers", "torch", "sentencepiece", "pipeline", "bs4", "requests", "tensorflow"), pip = TRUE)
```

###2. Environmental setup 

```{r}

sentencepiece <- import("sentencepiece")
tensorflow <- import("tensorflow")
transformers <- import("transformers")
requests <- import("requests")
PegasusTokenizer <- import("transformers", "PegasusTokenizer")
PegasusForConditionalGeneration <- import("transformers", "PegasusForConditionalGeneration")
TFPegasusForConditionalGeneration <- import("transformers", "TFPegasusForConditionalGeneration")
BeautifulSoup <- import("bs4", "BeautifulSoup")
pipeline <- import("transformers", "pipeline")

```

### 3.Installing pre-requisites for the transformers
```{r}
# Loading reticulate to the current R session
library(keras)
library(tensorflow)
library(dplyr)
library(tfdatasets)

transformer = reticulate::import('transformers')

tf$keras$backend$set_floatx('float32')
# Retrieve/force initialization of Python
reticulate::py_config()
reticulate::py_available()
```

### 4. Calling the model from hugging face
```{r}

model_name = "human-centered-summarization/financial-summarization-pegasus"
# get Tokenizer
tokenizer <- transformer$PegasusTokenizer$from_pretrained(model_name,do_lower_case=TRUE)
# get Model
model <- transformers$PegasusForConditionalGeneration$from_pretrained(model_name)

```


### 5. This Code slice focuses on the Automation of more than one stock symbol

```{r}
library(rvest)

search_for_stock_news_urls <- function(ticker, source) {
  if (source == "Bloomberg") {
    search_url <- paste0("https://www.google.com/search?q=bloomberg+", ticker, "&tbm=nws")
  } else if (source == "Yahoo") {
    search_url <- paste0("https://www.google.com/search?q=yahoo+finance+", ticker, "&tbm=nws")
  } else if (source == "Investopedia") {
    search_url <- paste0("https://www.google.com/search?q=investopedia+", ticker, "&tbm=nws")
  } else if (source == "Google Finance") {
    search_url <- paste0("https://www.google.com/search?q=google+finance++", ticker, "&tbm=nws")
  } else {
    return(NULL)
  }
  
  r <- read_html(search_url)
  atags <- r %>% html_nodes("a")
  hrefs <- atags %>% html_attr("href")
  return(hrefs)
}

num_tickers <- as.integer(readline("Enter the number of stock tickers you want to monitor: "))
monitored_tickers <- vector()
for (i in 1:num_tickers) {
  ticker <- readline(paste0("Enter the company ticker ", i, ": "))
  monitored_tickers <- c(monitored_tickers, ticker)
}

sources <- c("Bloomberg", "Yahoo", "Investopedia", "Google Finance")
source_choice <- readline(paste0("Enter the source you want to use (", paste(sources, collapse = ", "), "), or type 'all': "))

if (tolower(source_choice) == "all") {
  raw_urls <- list()
  for (ticker in monitored_tickers) {
    raw_urls[[ticker]] <- list()
    for (source in sources) {
      urls <- search_for_stock_news_urls(ticker, source)
      if (!is.null(urls)) {
        raw_urls[[ticker]][[source]] <- urls
      }
    }
  }
} else {
  raw_urls <- lapply(monitored_tickers, function(ticker) {
    urls <- search_for_stock_news_urls(ticker, source_choice)
    return(urls)
  })
  names(raw_urls) <- monitored_tickers
}

print(raw_urls)

```

### 6. Stripping the unwanted URLs
```{R}

library(stringr)

excluded_list <- c('maps', 'policies', 'preferences', 'support', 'accounts')

strip_unwanted_urls <- function(urls, excluded_list) {
  final_val <- character(0)
  for (url in urls) {
    if (str_detect(url, "https://") && !any(str_detect(url, excluded_list))) {
      res <- str_extract(url, "https://\\S+") %>% str_split("&") %>% .[[1]]
      final_val <- c(final_val, res)
    }
  }
  final_val <- final_val[str_sub(final_val, -5) == ".html"]
  return(unique(final_val))
}


cleaned_urls <- lapply(raw_urls, function(urls) {
  cleaned <- strip_unwanted_urls(urls, excluded_list)
  return(cleaned)
})
names(cleaned_urls) <- monitored_tickers
cleaned_urls

```
### 7. Scrapping and cleaning the URLs for the paragraphs

```{R}

library(rvest)

scrape_and_process <- function(urls) {
  ARTICLES <- vector("character", length = length(urls))
  
  for (i in seq_along(urls)) {
    r <- read_html(urls[i])
    paragraphs <- r %>% html_nodes("p")
    text <- sapply(paragraphs, html_text)
    words <- unlist(strsplit(paste(text, collapse = ""), " ", fixed = TRUE))[1:400]
    ARTICLE <- paste(words, collapse = "")
    ARTICLES[i] <- ARTICLE
  }
  
  return(ARTICLES)
}

articles <- lapply(cleaned_urls, scrape_and_process)
names(articles) <- monitored_tickers
articles


```


### 8. Calling the Facebook summaries from Facebook BART

```{r}

# Define the summarization pipeline
summarizer <- transformer$pipeline("summarization", model="facebook/bart-large-cnn")

# Function to summarize all articles
summarize_all_articles <- function(articles){
  summaries <- vector(mode = "list", length = length(articles))
  
  for (i in seq_along(articles)){
    # Use the summarizer pipeline to generate a summary
    summary <- summarizer(articles[i], max_length=120, min_length=30L, do_sample=FALSE)[[1]]$summary_text
    summaries[[i]] <- summary
  }
  return(summaries)
}
# Get summaries for each ticker using the function
summaries <- lapply(articles, summarize_all_articles)

# Convert the result to a named list
names(summaries) <- monitored_tickers

# Print the output
summaries


```


###9. Sentiment analysis on summaries 

```{r}

# Define the model and tokenizer
model_name <- "distilbert-base-uncased-finetuned-sst-2-english"
model_revision <- "af0f99b"


model <- transformers$AutoModelForSequenceClassification$from_pretrained(model_name, revision=model_revision)
tokenizer <- transformers$AutoTokenizer$from_pretrained(model_name, revision=model_revision)

# Define the sentiment analysis pipeline
sentiment <- transformer$pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# Call the pipeline to perform sentiment labelling for the summaries
# Assuming that the 'summaries' variable from the previous code block is still in memory
sentiment_labels <- lapply(summaries, function(ticker_summaries){
  lapply(ticker_summaries, function(summary){
    result <- sentiment(summary)[[1]]
    list(label=result$label, score=result$score)
  })
})

# Convert the result to a named list
names(sentiment_labels) <- monitored_tickers

# Print the output
sentiment_labels


```


###10. Final, Outputting negative labels

```{r}

# Define the stop words and phrases
stop_words <- c('Ourengineersareworkingquicklytoresolvetheissue. Thankyouforyourpatience. Back to Mail Online home. back to the page you came from.', '2023-私隱權政策-條款. ©2023. www.japanewspaper.com')

# Define a function to count the sentiment labels
count_sentiment_labels <- function(scores) {
  sentiment_counts <- c('NEGATIVE' = 0, 'NEUTRAL' = 0, 'POSITIVE' = 0)
  for (score in scores) {
    label <- score[['label']]
    if (label %in% names(sentiment_counts)) {
      sentiment_counts[[label]] <- sentiment_counts[[label]] + 1
    }
  }
  return(sentiment_counts)
}

# Loop through the tickers and print the sentiment analysis results
for (ticker in monitored_tickers) {
  cat(paste0('\nSentiment analysis for ', ticker, ':\n'))
  for (i in seq_along(scores[[ticker]])) {
    score <- scores[[ticker]][[i]]
    summary <- summaries[[ticker]][[i]]
    if (any(grepl(stop_words, summary)) || grepl('^\\s*$', summary)) {
      next  # Skip if the sentiment contains stop words or phrases, or is blank
    }
    cat(paste0(i, '. Summary: ', summary, '\n'))
    cat(paste0('   Score: ', score[['score']], ', Label: ', score[['label']], '\n'))
    sentiment_counts <- count_sentiment_labels(scores[[ticker]])
  }
  negative_count <- sentiment_counts[['NEGATIVE']]
  if (negative_count > 5) {
    cat(paste0('\nThe model finds that stock ', ticker, ' is not doing well currently. We recommend that you don\'t buy for short holding.\n'))
  } else if (negative_count == 5) {
    cat(paste0('\nThe model generates a neutral view on stock ', ticker, ', further research is needed: Bloomberg / Expedia refinement.\n'))
  } else if (negative_count < 5) {
    cat(paste0('\nThe model finds that stock ', ticker, ' is good to buy for the short term. Contact our brokers to buy.\n'))
  }
}

# Count the sentiment labels across all tickers
sentiment_counts <- c('NEGATIVE' = 0, 'NEUTRAL' = 0, 'POSITIVE' = 0)
for (ticker in monitored_tickers) {
  sentiment_counts <- sentiment_counts + count_sentiment_labels(scores[[ticker]])
}

# Visualize the sentiment counts using a bar chart
barplot(sentiment_counts, main = 'Sentiment Analysis Results', xlab = 'Sentiment Label', ylab = 'Count')


```
